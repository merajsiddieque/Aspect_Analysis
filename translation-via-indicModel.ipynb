{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11247619,"sourceType":"datasetVersion","datasetId":7028047},{"sourceId":11310850,"sourceType":"datasetVersion","datasetId":7074195},{"sourceId":11323949,"sourceType":"datasetVersion","datasetId":7082891},{"sourceId":11324070,"sourceType":"datasetVersion","datasetId":7082981},{"sourceId":11324459,"sourceType":"datasetVersion","datasetId":7083275}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# First clone inside Datasets Section inside Kaggle\n# https://github.com/VarunGumma/IndicTransToolkit/tree/main/IndicTransToolkit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:24:35.858394Z","iopub.execute_input":"2025-04-09T16:24:35.858661Z","iopub.status.idle":"2025-04-09T16:24:36.262885Z","shell.execute_reply.started":"2025-04-09T16:24:35.858640Z","shell.execute_reply":"2025-04-09T16:24:36.261986Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/firebase-key/sentiment-analysis-664f5-firebase-adminsdk-fbsvc-70f56e3d9b.json\n/kaggle/input/sentiment-dataset/training-data.txt\n/kaggle/input/sentiment-dataset/test-data (1).txt\n/kaggle/input/labels-datasets/training-data-telugu-labels.txt\n/kaggle/input/labels-datasets/training-data-marathi-labels.txt\n/kaggle/input/labels-datasets/training-data-labels.txt\n/kaggle/input/translated-datasets/test-data-marathi.txt\n/kaggle/input/translated-datasets/test-data-telugu.txt\n/kaggle/input/translated-datasets/training-data-marathi.txt\n/kaggle/input/translated-datasets/training-data-telugu.txt\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/LICENSE\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/.gitignore\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/pyproject.toml\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/README.md\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/requirements.txt\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/CHANGELOG.md\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/setup.py\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/processor.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/version.txt\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/version.py\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/processor.pyx\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/collator.py\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/processor.c\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/evaluator.py\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/IndicTransToolkit/__init__.py\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/build/temp.linux-x86_64-cpython-310/IndicTransToolkit/fast_processor.o\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/build/temp.linux-x86_64-cpython-310/IndicTransToolkit/processor.o\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/build/lib.linux-x86_64-cpython-310/IndicTransToolkit/processor.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/indictranstoolkit/IndicTransToolkit-main/build/lib.linux-x86_64-cpython-310/IndicTransToolkit/fast_processor.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/testsss/unlabeled_reviews.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import firebase_admin\nfrom firebase_admin import credentials, firestore\n\n# ✅ Path to your Admin SDK key\nservice_account_path = \"/kaggle/input/firebase-key/sentiment-analysis-664f5-firebase-adminsdk-fbsvc-70f56e3d9b.json\"\n\n# ✅ Initialize Firebase only if not already initialized\nif not firebase_admin._apps:\n    cred = credentials.Certificate(service_account_path)\n    firebase_admin.initialize_app(cred)\n\n# ✅ Connect to Firestore\ndb = firestore.client()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:50:22.694876Z","iopub.execute_input":"2025-04-09T05:50:22.695174Z","iopub.status.idle":"2025-04-09T05:50:22.699670Z","shell.execute_reply.started":"2025-04-09T05:50:22.695149Z","shell.execute_reply":"2025-04-09T05:50:22.698549Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ✅ Fetch the latest document from \"Raw-Data\"\ndocs = db.collection('Raw-Data').order_by(\"uploadedAt\", direction=firestore.Query.DESCENDING).limit(1).stream()\n\nfor doc in docs:\n    data = doc.to_dict()\n    session_id = data.get(\"sessionId\", \"unknown\")\n    filename = data.get(\"filename\", \"user_input.txt\")\n    raw_text = data.get(\"content\", \"\")\n\n    # ✅ Save the content to a .txt file\n    output_path = f\"/kaggle/working/web-user-input.txt\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(raw_text)\n\n    print(\"✅ File retrieved and saved successfully.\")\n    print(f\"📁 Session ID: {session_id}\")\n    print(f\"📄 Saved as: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T05:50:42.749810Z","iopub.execute_input":"2025-04-09T05:50:42.750087Z","iopub.status.idle":"2025-04-09T05:50:42.830405Z","shell.execute_reply.started":"2025-04-09T05:50:42.750066Z","shell.execute_reply":"2025-04-09T05:50:42.829765Z"}},"outputs":[{"name":"stdout","text":"✅ File retrieved and saved successfully.\n📁 Session ID: 7b9b45a9-32ac-40d8-b732-c469230252f4\n📄 Saved as: /kaggle/working/web-user-input.txt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n!cp -r /kaggle/input/indictranstoolkit/IndicTransToolkit-main /kaggle/working/IndicTransToolkit\n%cd /kaggle/working/IndicTransToolkit\n!pip install -e .\n%cd /kaggle/working\nimport sys\nsys.path.append('/kaggle/working/IndicTransToolkit')\nfrom IndicTransToolkit import evaluator, collator\nfrom IndicTransToolkit import IndicProcessor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:24:53.231547Z","iopub.execute_input":"2025-04-09T16:24:53.231951Z","iopub.status.idle":"2025-04-09T16:25:35.680028Z","shell.execute_reply.started":"2025-04-09T16:24:53.231927Z","shell.execute_reply":"2025-04-09T16:25:35.679151Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/IndicTransToolkit\nObtaining file:///kaggle/working/IndicTransToolkit\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git (from IndicTransToolkit==1.0.3)\n  Cloning https://github.com/VarunGumma/indic_nlp_library.git to /tmp/pip-install-4veipexy/indic-nlp-library-it2_8919abf38af9419eaa91490a1c81abcd\n  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library.git /tmp/pip-install-4veipexy/indic-nlp-library-it2_8919abf38af9419eaa91490a1c81abcd\n  Resolved https://github.com/VarunGumma/indic_nlp_library.git to commit 342a7e95735e88949528bdca371518cb0ba5bb5d\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.3) (75.1.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.3) (2.5.1+cu121)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.3) (3.0.11)\nCollecting sacremoses (from IndicTransToolkit==1.0.3)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.3) (0.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.3) (4.47.0)\nCollecting sacrebleu (from IndicTransToolkit==1.0.3)\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3)\n  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: sphinx_rtd_theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (0.2.4)\nCollecting morfessor (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.26.4)\nCollecting portalocker (from sacrebleu->IndicTransToolkit==1.0.3)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.3) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.3) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.3) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.3) (5.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.3) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.3) (1.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.3) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->IndicTransToolkit==1.0.3) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.3) (0.4.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->IndicTransToolkit==1.0.3) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.3) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.3) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.3) (2025.1.31)\nRequirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (8.1.3)\nRequirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (0.21.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.17.0)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.0.0)\nRequirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.19.1)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.16.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.4.1)\nRequirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2.2.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library.git->IndicTransToolkit==1.0.3) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: IndicTransToolkit, indic-nlp-library-IT2\n  Building editable for IndicTransToolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for IndicTransToolkit: filename=indictranstoolkit-1.0.3-0.editable-cp310-cp310-linux_x86_64.whl size=6244 sha256=e96163e331f63c35e4afede6428c577559085eb153e4156facf4a5c5fdc269c7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-pyezn9_1/wheels/08/d4/a9/f5882323c250c712e4f351deac19172c80fde4bb092c180785\n  Building wheel for indic-nlp-library-IT2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for indic-nlp-library-IT2: filename=indic_nlp_library_IT2-0.0.2-py3-none-any.whl size=48116 sha256=af2d9f6adf7423671abcd181dc5f1a945522f06436597aac4ce8dd614cebcd5b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-pyezn9_1/wheels/dd/35/2f/193234de1249b73bbaa65ffdf225a2e01d83e4419122203af5\nSuccessfully built IndicTransToolkit indic-nlp-library-IT2\nInstalling collected packages: morfessor, sacremoses, portalocker, sphinx-argparse, sacrebleu, indic-nlp-library-IT2, IndicTransToolkit\nSuccessfully installed IndicTransToolkit-1.0.3 indic-nlp-library-IT2-0.0.2 morfessor-2.0.6 portalocker-3.1.1 sacrebleu-2.5.1 sacremoses-0.1.1 sphinx-argparse-0.5.2\n/kaggle/working\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit import IndicProcessor\nfrom tqdm import tqdm\n\n# ==== DEVICE CONFIG ====\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ==== LANGUAGE CONFIG ====\nsrc_lang, tgt_lang = \"hin_Deva\", \"tel_Telu\"  # Set target lang if needed\n\n# ==== MODEL & TOKENIZER ====\nmodel_name = \"ai4bharat/indictrans2-indic-indic-dist-320M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    attn_implementation=\"flash_attention_2\"\n).to(DEVICE)\n\nip = IndicProcessor(inference=True)\n\n# ==== INPUT FILE CONFIG ====\ninput_file = \"/kaggle/working/training-data-labels.txt\"\noutput_file = \"/kaggle/working/training-data-labels-telugu.txt\"\n\n# ==== READ INPUT ====\nwith open(input_file, \"r\", encoding=\"utf-8\") as f:\n    input_sentences = [line.strip() for line in f if line.strip()]\n\n# ==== TRANSLATE IN BATCHES ====\ntranslations = []\nbatch_size = 4  # Increase if memory allows\n\nfor i in tqdm(range(0, len(input_sentences), batch_size), desc=\"Translating\"):\n    input_batch = input_sentences[i:i+batch_size]\n\n    # Preprocess\n    batch = ip.preprocess_batch(\n        input_batch,\n        src_lang=src_lang,\n        tgt_lang=tgt_lang,\n    )\n\n    # Tokenize\n    inputs = tokenizer(\n        batch,\n        truncation=True,\n        padding=\"longest\",\n        return_tensors=\"pt\",\n        return_attention_mask=True,\n    ).to(DEVICE)\n\n    # Generate\n    with torch.no_grad():\n        generated_tokens = model.generate(\n            **inputs,\n            use_cache=True,\n            min_length=0,\n            max_length=256,\n            num_beams=5,\n            num_return_sequences=1,\n        )\n\n    # Decode\n    with tokenizer.as_target_tokenizer():\n        decoded = tokenizer.batch_decode(\n            generated_tokens.detach().cpu().tolist(),\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n\n    # Postprocess\n    translations.extend(ip.postprocess_batch(decoded, lang=tgt_lang))\n\n    # Free memory\n    del inputs, generated_tokens\n    torch.cuda.empty_cache()\n\n# ==== WRITE OUTPUT ====\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    for input_sentence, translation in zip(input_sentences, translations):\n        # f.write(f\"{src_lang}: {input_sentence}\\n\")\n        f.write(f\"{translation}\\n\")\n\nprint(f\"✅ Translations written to: {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:38:57.376632Z","iopub.execute_input":"2025-04-09T17:38:57.377233Z","iopub.status.idle":"2025-04-09T17:40:24.719657Z","shell.execute_reply.started":"2025-04-09T17:38:57.377191Z","shell.execute_reply":"2025-04-09T17:40:24.718906Z"}},"outputs":[{"name":"stderr","text":"Translating: 100%|██████████| 451/451 [01:25<00:00,  5.26it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Translations written to: /kaggle/working/training-data-labels-telugu.txt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# extracting labels from training data \ninput_path = \"/kaggle/input/sentiment-dataset/training-data.txt\"  # <-- adjust this to your dataset\noutput_path = \"/kaggle/working/training-data-labels.txt\"\n\n# Read the data\nwith open(input_path, 'r', encoding='utf-8') as file:\n    lines = [line.strip() for line in file if line.strip()]\n\n# Extract labels (assuming last word in each line is the label)\nlabels = [line.split()[-1] for line in lines]\n\n# Save labels to output file\nwith open(output_path, 'w', encoding='utf-8') as out_file:\n    for label in labels:\n        out_file.write(label + '\\n')\n\nprint(f\"✅ Labels extracted and saved to: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:33:39.626289Z","iopub.execute_input":"2025-04-09T17:33:39.626620Z","iopub.status.idle":"2025-04-09T17:33:39.678393Z","shell.execute_reply.started":"2025-04-09T17:33:39.626598Z","shell.execute_reply":"2025-04-09T17:33:39.677534Z"}},"outputs":[{"name":"stdout","text":"✅ Labels extracted and saved to: /kaggle/working/training-data-labels.txt\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom pickle import dump\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nimport re\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom random import shuffle\n\n\ndef readLinesFromFile(filePath):\n    with open(filePath, 'r', encoding='utf-8') as fileRead:\n        return [line.strip() for line in fileRead.readlines() if line.strip()]\n\n\ndef createTFIDFVectorsFromTrainData(trainData, analyzer='word', ngram_range=(1, 1)):\n    tfIDFVect = TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range)\n    trainTFIdf = tfIDFVect.fit_transform(trainData)\n    return trainTFIdf, tfIDFVect\n\n\ndef dumpObjectIntoFile(filePath, dataObject):\n    with open(filePath, 'wb') as fileDump:\n        dump(dataObject, fileDump)\n\n\ndef fitTrainDataWithClassifier(clf, trainData, trainLabels):\n    clf.fit(trainData, trainLabels)\n    return clf\n\n\ndef SVMClassifier(trainData, trainLabels):\n    svm = LinearSVC()\n    return fitTrainDataWithClassifier(svm, trainData, trainLabels)\n\n\ndef logisticClassifier(trainData, trainLabels):\n    logit = LogisticRegression()\n    return fitTrainDataWithClassifier(logit, trainData, trainLabels)\n\n\ndef gradientDescent(trainData, trainLabels):\n    sgd = SGDClassifier(loss='perceptron')\n    return fitTrainDataWithClassifier(sgd, trainData, trainLabels)\n\n\ndef gradientBoost(trainData, trainLabels):\n    gradBoost = GradientBoostingClassifier()\n    return fitTrainDataWithClassifier(gradBoost, trainData, trainLabels)\n\n\ndef multinomialNBClassifier(trainData, trainLabels):\n    mNB = MultinomialNB(alpha=0.1)\n    return fitTrainDataWithClassifier(mNB, trainData, trainLabels)\n\n\ndef main():\n    # ✅ Direct file paths (Kaggle)\n    dataFilePath = '/kaggle/input/combined-datasets/hindi-marathi-telugu-training-data.txt'\n    labelFilePath = '/kaggle/input/combined-datasets/hindi-marathi-telugu-labels.txt'\n    classifier = 'svm'  # 👈 You can change this to 'logistic', 'multi-nb', 'sgd', etc.\n\n    char_analyzer = 'char'\n    char_ngram_range = (2, 5)\n    word_analyzer = 'word'\n    word_ngram_range = (1, 1)\n\n    trainData = readLinesFromFile(dataFilePath)\n    trainLabels = readLinesFromFile(labelFilePath)\n\n    assert len(trainLabels) == len(trainData)\n\n    indexes = list(range(len(trainLabels)))\n    shuffle(indexes)\n    trainData = [trainData[i] for i in indexes]\n    trainLabels = [trainLabels[i] for i in indexes]\n\n    wordTrainTFIdf, wordTfIdfVect = createTFIDFVectorsFromTrainData(trainData, word_analyzer, word_ngram_range)\n    charTrainTFIdf, charTfIdfVect = createTFIDFVectorsFromTrainData(trainData, char_analyzer, char_ngram_range)\n    combinedTrainTfIdf = hstack([wordTrainTFIdf, charTrainTFIdf])\n\n    if re.search('svm', classifier, re.I):\n        classifierToSelect = SVMClassifier(combinedTrainTfIdf, trainLabels)\n    elif re.search('logistic', classifier, re.I):\n        classifierToSelect = logisticClassifier(combinedTrainTfIdf, trainLabels)\n    elif re.search('multi-nb', classifier, re.I):\n        classifierToSelect = multinomialNBClassifier(combinedTrainTfIdf, trainLabels)\n    elif re.search('sgd', classifier, re.I):\n        classifierToSelect = gradientDescent(combinedTrainTfIdf, trainLabels)\n    elif re.search('gradient-boosting', classifier, re.I):\n        classifierToSelect = gradientBoost(combinedTrainTfIdf, trainLabels)\n    else:\n        raise ValueError(\"Unsupported classifier selected.\")\n\n    # ✅ Save outputs to /kaggle/working/\n    dumpObjectIntoFile(f'/kaggle/working/train-vect-word-{classifier}.pkl', wordTfIdfVect)\n    dumpObjectIntoFile(f'/kaggle/working/train-vect-char-{classifier}.pkl', charTfIdfVect)\n    dumpObjectIntoFile(f'/kaggle/working/classifier-{classifier}.pkl', classifierToSelect)\n\n    print(\"✅ Model and vectorizers saved to /kaggle/working/\")\n\n\n# ✅ Run it\nmain()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:52:05.952868Z","iopub.execute_input":"2025-04-09T17:52:05.953305Z","iopub.status.idle":"2025-04-09T17:52:08.749111Z","shell.execute_reply.started":"2025-04-09T17:52:05.953274Z","shell.execute_reply":"2025-04-09T17:52:08.748350Z"}},"outputs":[{"name":"stdout","text":"✅ Model and vectorizers saved to /kaggle/working/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom pickle import load\n\ndef readLinesFromFile(filePath):\n    with open(filePath, 'r', encoding='utf-8') as fileRead:\n        return [line.strip() for line in fileRead.readlines() if line.strip()]\n\ndef createTestTfIdf(testData, tfIdfVect):\n    return tfIdfVect.transform(testData)\n\ndef loadObjectFromFile(filePath):\n    with open(filePath, 'rb') as fileLoad:\n        return load(fileLoad)\n\ndef writeListToFile(filePath, dataList):\n    with open(filePath, 'w', encoding='utf-8') as fileWrite:\n        fileWrite.write('\\n'.join(dataList))\n\ndef predictOnFeatures(classifier, features):\n    return classifier.predict(features)\n\n# 🔽 Paths\ntestFilePath = '/kaggle/input/translated-datasets/training-data-telugu.txt'\nwordVectPath = '/kaggle/working/train-vect-word-svm.pkl'\ncharVectPath = '/kaggle/working/train-vect-char-svm.pkl'\nclassifierPath = '/kaggle/working/classifier-svm.pkl'\noutputPredPath = '/kaggle/working/test-predictions-telugu-2.txt'\n\n# 🔄 Prediction steps\ntestData = readLinesFromFile(testFilePath)\nwordVect = loadObjectFromFile(wordVectPath)\ncharVect = loadObjectFromFile(charVectPath)\nclf = loadObjectFromFile(classifierPath)\n\nwordTestTfIdf = createTestTfIdf(testData, wordVect)\ncharTestTfIdf = createTestTfIdf(testData, charVect)\ncombinedTestTfIdf = hstack([wordTestTfIdf, charTestTfIdf])\n\npredictions = predictOnFeatures(clf, combinedTestTfIdf)\n\nwriteListToFile(outputPredPath, predictions)\n\nprint(\"✅ Predictions saved to:\", outputPredPath)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:48:31.704348Z","iopub.execute_input":"2025-04-09T16:48:31.704662Z","iopub.status.idle":"2025-04-09T16:48:32.837367Z","shell.execute_reply.started":"2025-04-09T16:48:31.704639Z","shell.execute_reply":"2025-04-09T16:48:32.836570Z"}},"outputs":[{"name":"stdout","text":"✅ Predictions saved to: /kaggle/working/test-predictions-telugu-2.txt\n","output_type":"stream"}],"execution_count":9}]}